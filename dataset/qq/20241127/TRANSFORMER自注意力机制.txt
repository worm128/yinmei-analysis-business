有用TRANSFORMER预测金融资产价格的吗？
嗯，特别高大上。。。。其实就是预测股价了。。。
小模型预测不了
4万多根数据。或者可以更多一点，60万根数据
不够吧， 这种推理性预测 普通模型没有出现涌现能力之前都不准确
起码60亿的模型，推理出来的才具备点准确性
60亿条数据？
60亿参数
光有数据不够
噢。吓我一跳。
参数量和数据量之间也有关联
近似白噪声预测不了的。除非你打高频追涨杀跌，或者对市场有独到的见解
然后就是 loss调优， loss逼近1以下， 测出来的数据应该能具备一些市场经验
行情肯定不是白噪音了。可以去掉一些白噪音数据，然后就是有明确的上涨和下跌的趋势数据的。
可以，想训练出推理能力的模型，普通用户根本用不起这算力
我还是回用去马尔科夫链吧。
没出来涌现能力的模型，还是统计那一套俗称乱猜
一些概率分布。
从原理上来说，还是马尔科链感觉靠谱一点。
需要的算力还好，而且时序上xgboost 还是很能打的
<100B的模型 + 上好的大规模的训练数据 + 强大算力 + 耐心调优  = 优质推理模型
一个是白盒，一个纯黑盒子
transformer的原理究竟是啥。
首先，要确定，这样的模型是否适合处理这些其实是有一定贝叶思条件概率但随后基本上是随机漫步的数据
否则岂不是白瞎。。。类似让眼晴是听声音之类的。
这个工具是否适合这些数据，首先是个假设。。。。
位置编码+词嵌入+自注意力机制+前馈神经+残差连接和层归一化+自掩码
感觉你们都在处理文本。文本是网状的结构。
金融数据没有这种网状的关系。
本质和常规学习一样是做特征提取， 不过中间有很多层多头自注意力，可以捕捉长数据直接的相互依赖，捕捉到就可以存在前馈记忆层
1月1号的行情数据跟三个月前10月1号的行情数据基本上是毫无关系的。
离得越远就一点关系都没有。
再好的预测也是滑动平均，预测股价预测收益都不靠谱。还是看个人的策略
最大的特征就是离得越远离没关系。
个人策略早就跑通了。。。
哈哈。我只是想加点啥，看看是否可以有更好的预测。
哈哈脱离市场大环境的纯数据，就算是人
算了算了。听下来太高级的工具。我还是去找马尔科夫链的应用吧。
也分析不出来
确实是这样的。
你还的给他喂市场经济，全球经济实况。 数据
大周期会复现，如2008-2010，可能类似2018-2020。
还有人性， 策略
这种已经不是个人能训练的模型了
这特么是构造一个上帝了。
算了算了。太吓人。
gpt5马上来了， 同说是具备超强推理的模型
我要是能造出个能预测的上帝来。。。。。不如现在就躺平了。。。
1， 我们不具备优质且庞大的数据集，2.我们不具备算力 3.我们不具备商业环境，无法吸引投资
解决3 才能解决 1和2的问题
transformer的实体命名，能标注一段描述吗，比较长的一个句子描述
doccano好像标注不了字符长的
好像是浏览器的问题，谷歌浏览器可以了
现在不是说液体模型要替代transformer了吗 MIT系初创公司Liquid AI推出液体基础模型LFM，1B模型在基准测试中成为SOTA，这是非GPT架构首次显著超越Transformer模型！
液体模型什么意思
请问有用transformer做过时序预测的大佬吗
itransfomer？
是这个吗
做过分类
实体命名
刚开始学，感觉好难
这nlp的时间范围太难规定了
我之前说了什么
我前天说了什么
我大前天说了什么
我10月份说了什么
我10月2号说了什么
我上次说了什么
我上回说了什么
我那天说了什么
这种一堆的回忆语句，感觉很多情况要处理
We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions.
可以解释一下这个是什么意思吗？
什么教inverted dimensions
是将序列反过来预测的意思吗？
先输出后面的再输出前面的？
我还不会，正在学这个
iTransformer
大佬们我有个问题，Vision Transformer的cls_token是不加在token 序列里面，而是和token并列成一个新的序列，还能做到全局交互吗？
比如说这样
合并起来一起计算，分类的时候再单独取出来
 感谢大佬的回答，容许我再问一下，我看网上解释的大多都是放在token其他位置，比如说放在中间或者末尾，但是我这个这个类似于ViT分成9个patch嘛，然后我让Cls_token单独做了个新的Patch，这样也能交互嘛？
我当时大概这么画了一下Q*K，发现最后那个元素实际是cls乘cls，就是没任何信息
放在哪里都可以，每个token 都能获得全局信息，attention 是并行计算的。从放入的地方取出来就行了，一般为了方便都放第一个或最后一个
不不不，没放在token里，类似于和token序列并行的一个新序列，一个单独的clstoken序列，也可以的嘛？
创新吗？
不是，是模型这块序列位置弄错了，想加在序列里，但是加在patch唯独了，不知道对不对
没听懂，参与qkv 计算就交互了，没参与就没有交互
但是是不是得保证在q*k*v过程中cls-token和其他元素都能保证相乘过
对，或者是其他和attention 类似计算相似度然后加权的过程
正常Vit过程比如是12*12的图片，分3*3的patch，应该分成，这样分成4*4个向量，每个向量有9个元素，就是16*9的形式。加入cls-token会加在元素上也就是会变成16*10的形式。但是我加在，向量维度了，17*9的形式了
大概知道你的意思了，你的两个例子是不是反了？
相当于每个patch里塞一个像素点吗？
对对对  这里却是反了，感谢大佬，我之前理解错了
感谢大佬的帮助！！！
大佬们，图神经网络里面都有什么统一数据维度的方法呀
啥意思
数据维度是特征数量吗
就比如吧
我的输入是长短不一的序列
怎样统一输入的不同序列的维度
呃呃不懂序列
那如果输入是比如（10×20）（10×30）的矩阵呢，可以用些什么方法把他两都转化为（10×30）
20 和30可以理解成特征数量
如果是特征数量 我知道怎么把30变成20
PCA呗
有没有人知道那种文本生成器的模型怎么制作呢？
这不太好
信息给人丢失了
那我没法了
想知道20怎么到30
transformer做图像识别
预测
有偿
2000左右 简单 有人接吗
预测什么
图像预测
ai 都可以做到这么便宜了？

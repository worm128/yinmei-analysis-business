64毫秒
训练了几条数据就这么准了
没有论文idea，论文无从下手?
超500位QS前50的教授/博士 , 辅导idea设计、实验设计、论文写作等。
扫码免费咨询！助你成功发刊！
用户回复：请重复我上次的发言，并点歌
这种聊天应该怎样做文本分析，让ai可以知道他要点歌，又要识别他是从历史记录抽取
label要叠加标签吗
大家好这里是讨论transformer的吗？
我想研究transformer的涌现机制，不知道有没有谁可以推荐一下参考资料
我想的是训练一个模型，让它可以学会任意数的加法
由于我们不可能让它看完所有的加法的文本，那么模型一定是总结出了加法的规律
比如进位，对齐个位数等等的
看看transformer模型是不是真正出现了智能的苗头，而不是语言的统计
softmax是统计。词向量，自注意力机制和前馈是特征提取
但是大模型据说能出现真正的通用智能agi, 这是怎么产生的呢？
你说的这些基本运算都是存在规律的，有规律就走特征。 只要训练数据足够，有特征就能被transformer识别
目前Transformer架构不太可能创造出agi
有规律就能识别，那岂不是可以找到宇宙的真理?我把所有的物理数据导入transformer，能推导出牛吨力学吗？
我觉得如果能找出加法的规律，就可以说明它有产生智能的潜力了。
是的，前提你的有
只要有规律，并且数据足够，特征就能找到
找到特征=理解
那就是说你相信一定能用transformer训练出一个加法器罗？
加法器？这玩意用监督学习模型都能学会吧？
大量加法运算的标记数据丢给模型，模型就会了
可以实现任意位数的加法器，应该不能用监督学习吧
任意位数不可能，目前计算机最多也就运算64位基地址+64位偏移地址 128位的数据
比如用transformer实现一个100位一下的10进制加法
如果一个小孩子的话，用足够的 时间，是很容易的，只要掌握了简单的规律
就看看大语言模型，能不能抽取到本质的特征
这纯属脱裤子放屁，计算器就能做啊，既然能编程出来，你说它能不能做到
但是我觉得这是真正的智能，可以通过这个来窥探智能形成的蛛丝马迹
你错了，真正的AGI是指独立意识
就等于给你1本小学数学，有各种计算结果，你能不能自我总结出加法的竖式，其实这不容易的
数学都是总结出的规律，有规律本质上模型就可以学习到，如果学不到就是模型参数还不够多，训练样本不够
而推理出它知识库以外的数据理论就是agi的初步体现，这个能力叫做推理
但是人，可能只看到过5位数一下的加法，就能计算100位的加法，这种能力，会不会也存在于transformer的结构中呢？
也是人的一项了不起的能力， 总结经验，创造新的知识
你要教ai使用公式计算，和小朋友一样，但是效率极低
不能让硅基生命的思维和人一样，才叫智能
我觉得目前的Transformer架构是有问题的， 每一次计算消耗都是指数增长
我们有一种想法，说人可能也是一个大模型，大脑说不能是一个用神经元做的大模型，所以我们可以探索一下大模型的能力，看看人脑的机制是不是同构的。
能用高效的二进制计算，绝不用低效的加法公式
不存在的，模拟人脑机制一定有更简单也不需要太消耗算力的方法，只是我们还没发现
其实计算本身就不是大模型的特长，它的特长是理解人，所以要研究transformer是不是和人脑同构，所以才能互相理解
虽然大脑这个也是算数，现在AI算法都是强算硬算啊，每一次都消耗大量计算资源
为啥大模型能耗这么大，而人脑这么少，原因科学家是有猜测的，ai之父hiton猜测是由于电脑的大模型是数字化的，所以是可以复制到硬盘上，所以可以是永生的。但是人脑是模拟的，不能完全复现，是会死的，保持永恒性是耗能的本质原因
有一定可信度，一个是模拟电路，一个是数字电路
估计做一个会死的大模型就可以实现大大节能
可能量子电脑可以实现，本质是每个instance都是独特的。
所以我觉得研究一下涌现是不是真的出现的人类智能是很有趣的事情
量子吗，本质是缩小了计算消耗，算法不变依然不会影响模型本质
你手头用什么硬件来玩transformer呢？
m1 pro
这个硬件可以训练出来吗？
训练个小模型应该问题不大
数据大的话跑个几天就行了
2021年的模型还不会基本的数学。不知道现在的模型是否真的懂了数学
关于合肥一夜成中国Ai前沿地，没有之一。各位怎么看
怎么得出这个结论的
去看小杨哥违法处理公告视频
录音门事件
不好说，我还以为啥事
你必须相信，因为这是"官方公告"， 从此合肥ai甲天下
真把人民群众当猴
doccano这个标注，把英文单词全部拆成了一个个字母，位置都错了
他界面又不能自定义分词
有活人吗
没有
都死人
嗯嗯，死啦死啦地
推荐一个与牛津、MT等顶尖高校合作的学术机构 ！
名校导师一对一指导，辅导至论文通过！
适合考研保研、留学申博、评职晋升
扫码为你匹配名师和论文逆袭方案！
技术不足、没实战？
是时候找一位大厂私人教练了
学AI、练项目、进大厂
梳理优势与不足！助你精准提升，快速拿到结果！
(可签协议保大厂offer)
没有论文idea，论文无从下手?
超500位QS前50的教授/博士 , 辅导idea设计、实验设计、论文写作等。
扫码免费咨询！助你成功发刊！
鲨鱼的手臂
有人最近在学习Transformer在时间序列方面的预测嘛
我想问下，如果我想用transformer的encoder层，我是不是在 pytorc 下直接引用就好了
不需要自己重新搭建了
来了! 顶会审稿人的学术论文指导服务！
超500位QS前50教授/博士,涵盖AI各领域 ;
没有idea,写作没思路,不知道润色的同学 ;
强烈推荐！ 扫码免费咨询！助你发刊！
导师放养，方向迷茫，论文无从下手?
别慌，超500位QS前50的教授/博士
辅导idea设计、实验设计、论文写作等等
扫码立即咨询，助你成功发刊！
我算交叉熵损失出这个是什么情况，序列的交叉熵损失我这么算有啥问题
大家好
AI领域CV、大模型等方向收学生，带发A会、B会！
有现成idea，可以1对1带发文章、辅导idea设计、实验设计、论文写作的。
导师都是顶会Best Paper得主、顶会领域主席、知名大厂研究科学家等。
想求带的，可以扫码了解  ,目前每个方向仅2个名额，先到先得！
我所理解神经网络的训练过程：
准备工作：
1、明确训练目标（分类问题通常使用交叉熵损失函数，而回归问题使用均方误差（MSE））
2、模型选择（Transformer、CNN、RNN、DNN）
3、数据预处理（标准化、归一化、数据增强）
4、如果训练的数据过多，数据满足降维的条件，可使用（PCA、SVD）对训练数据进行降维
开始训练流程：
1、输入层（第一层神经元）输入参数 和 偏置
2、隐藏层（第二层及后续神经元）
2.1、权重计算，神经网络的核心在于计算每层输入的加权和（线性代数运算，矩阵乘法）
2.2、激活函数，将权重计算过后的值带入激活函数（sigmoid、ReLU、Softmax、Tanh）
2.3、将当前神经元的输出作为下一层神经元的输入，并重复该步骤，直到倒数第二层
3、输出层（最后一层神经元）
3.1、计算损失函数（一般求当前值与实际值的均方误差 或 交叉熵损失）
3.2、梯度下降求偏导（在损失函数中求权重以及偏置的偏导数）
3.3、正则化，使用正则进行优化，防止过拟合（鼓励稀疏性，适用于特征选择 或 限制权重大小，防止过拟合）
3.4、反向传播，从输出层向隐藏层逐层更新权重和偏置
3.5、判断是否达到训练目的，未达到继续训练
我所理解神经网络训练过程，有没有大佬指点一下
可以的，一个比泛化的神经网络理解。因为不涉及具体的模型和具体的任务。所以整体流程是没问题的，可能有问题的点如下：1.输入层其实是个可训练的查询表，参数矩阵形状和运算结构是不同于全连接层的。 2.正则化可以根据需要出现一些结构之间，比如全连接层之后、卷积层之后、嵌入层之后、RNN/LSTM/GRU 等的隐藏状态之间、 Transformer 中的自注意力和前馈层之间
两个gpt人机？
